---
globs: "src/gha_optimizer/analyzers/**/*.py"
---

# AI Integration Rules for GHA-Optimizer

## Anthropic Claude API Only
- **ONLY use Anthropic Claude API** - No OpenAI, Google, or other AI providers
- Use the official `anthropic` Python client library
- Reference model configuration from [config.yml](mdc:config.yml)

## AI Analyzer Pattern
Follow the established pattern in [src/gha_optimizer/analyzers/ai_analyzer.py](mdc:src/gha_optimizer/analyzers/ai_analyzer.py):

```python
from anthropic import Anthropic
from typing import Dict, List, Optional

class AIAnalyzer:
    """Analyzes GitHub Actions workflows using Anthropic Claude."""
    
    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        """Initialize the AI analyzer with Anthropic client."""
        self.client = Anthropic(api_key=api_key)
        self.model = model
        
    def analyze_workflow(self, workflow_content: str, context: Dict[str, Any]) -> AnalysisResult:
        """
        Analyze a workflow using Claude AI.
        
        Args:
            workflow_content: YAML content of the workflow
            context: Additional context about the repository
            
        Returns:
            AnalysisResult with optimization suggestions
            
        Raises:
            AIAnalysisError: If AI analysis fails
        """
        try:
            # Construct prompt for Claude
            prompt = self._build_analysis_prompt(workflow_content, context)
            
            # Call Claude API
            response = self.client.messages.create(
                model=self.model,
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # Parse and return results
            return self._parse_analysis_response(response.content[0].text)
            
        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            raise AIAnalysisError(f"Failed to analyze workflow: {e}")
```

## Error Handling for AI Operations
Always handle AI API errors gracefully:

```python
try:
    response = self.client.messages.create(...)
except anthropic.APIError as e:
    logger.error(f"Anthropic API error: {e}")
    raise AIAnalysisError("AI service temporarily unavailable. Please try again later.")
except anthropic.RateLimitError as e:
    logger.error(f"Rate limit exceeded: {e}")
    raise AIAnalysisError("Rate limit exceeded. Please wait before retrying.")
except Exception as e:
    logger.error(f"Unexpected AI error: {e}")
    raise AIAnalysisError("AI analysis failed. Please check your API key and try again.")
```

## Prompt Engineering Guidelines
- Keep prompts clear and specific
- Include relevant context about the repository
- Ask for structured output that's easy to parse
- Focus on actionable optimization suggestions
- Limit response length to avoid token limits

## Configuration Pattern
AI configuration should reference [config.yml](mdc:config.yml):

```yaml
anthropic:
  api_key: ${ANTHROPIC_API_KEY}
  model: claude-3-sonnet-20240229
  max_tokens: 4000
  
optimization:
  focus_areas:
    - caching
    - parallelization
    - dependency_management
    - security
```

## Response Parsing
Structure AI responses for easy parsing:

```python
def _parse_analysis_response(self, response_text: str) -> AnalysisResult:
    """Parse Claude's response into structured data."""
    try:
        # Parse structured response
        suggestions = []
        # Implementation details...
        
        return AnalysisResult(
            suggestions=suggestions,
            confidence_score=confidence,
            analysis_timestamp=datetime.now()
        )
    except Exception as e:
        logger.error(f"Failed to parse AI response: {e}")
        raise AIAnalysisError("Failed to parse AI analysis results")
```